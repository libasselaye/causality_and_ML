{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CounterFactual Optimization for Bidding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import lognorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Exploration\n",
    "\n",
    "Read the dataset. `action` is the action and `y` is the reward. The action is a randomization around the deterministic bid and the reward is the value of a sale (if there is any for this user) minus the cost of the displays.\n",
    "\n",
    "1. Estimate the mean of the action under the current policy\n",
    "1. Do a linear regression on `y ~ action`. What do you observe ?\n",
    "1. Estimate the expectation of the reward by bucket of the action. What do you observe ? Look also at the variance in each bucket.\n",
    "1. Do you think it is possible to propose a better policy ? E.g. by bidding above/under the current average action ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('criteo-continuous-bandit-dataset.csv.1M.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counterfactual Reasoning on the bid level\n",
    "\n",
    "The randomization (action) follows a lognormal distribution, which parameters we'll learn now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_params = lognorm.fit(df.action)\n",
    "pa = lognorm(s=mle_params[0], loc=mle_params[1], scale=mle_params[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider a family of alternative policies instantiated as lognormal distributions with a different mean $\\alpha$. Our parameter $\\alpha$ will be restricted to the neighboorhood of the current policy e.g. [.8, 1.2], meaning that on average we would multiply our bids by this $\\alpha$ coefficient. \n",
    "\n",
    "NB: $\\alpha$ is not a direct parameter of the lognormal distribution, the code below computes the relevant $\\mu$ value so that $E[\\pi_{\\mu}] = \\alpha$\n",
    "\n",
    "1. Execute the code below to instantiate the family of policies $\\{\\pi_\\alpha\\}_{\\alpha \\in [.8;1.2]}$\n",
    "1. Plot $\\mu$ wrt $\\alpha$ \n",
    "1. Plot $w$, the corresponding importance weights. What property should they respect? Is that the case for all $\\alpha$? In your opinion, what should we do ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.linspace(.8, 1.2, 10)\n",
    "mus = []\n",
    "avgs = []\n",
    "ws = []\n",
    "for alpha in alphas:\n",
    "    mu_i = np.log(alpha) - mle_params[0]/2\n",
    "    mus += [mu_i]\n",
    "    pi = lognorm(s=mle_params[0], loc=mle_params[1], scale=np.exp(mu_i))\n",
    "    ws += [np.mean(pi.pdf(df.action)/pa.pdf(df.action))]\n",
    "    avg = pi.rvs(size=10**5).mean()\n",
    "    avgs += [avg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-order optimization of the bid level\n",
    "\n",
    "The goal now is to find the best possible policy (= best $\\alpha$ wrt $E_{\\pi_\\alpha}[Y]$) by grid-search over the possible values. For this we will use the following code that implements importance sampling, including estimation of the variance of the reward.\n",
    "\n",
    "1. choose an appropriate grid for $\\alpha$\n",
    "1. compute the corresponding parameters of the lognormal as in the previous section\n",
    "1. estimate $E_{\\pi_\\alpha}[Y]$ using the code; do you find a better policy ?\n",
    "1. pick your best $\\alpha$ and use `nb_bootstraps=30` to generate a distribution of the reward; transform it into a confidence interval for $E_{\\pi_\\alpha}[Y]$. Is the reward of the original policy inside ?\n",
    "1. redo the same but use the clipping parameter, e.g. ̀̀`clip=1000`; what do you observe ? are you confident on the result ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_weights(w, clip='bottou', verbose=False):\n",
    "    w_ = w.copy()\n",
    "    if clip is not None:\n",
    "        if clip == 'bottou':\n",
    "            clip = float(np.sort(w)[-5])\n",
    "        elif type(clip) in (int, float):\n",
    "            w = np.clip(w, None, clip)    \n",
    "        else:\n",
    "            raise ValueError('unsupported clipping value:', clip)\n",
    "    if verbose:\n",
    "        print(\"clip rmse:\", np.sqrt(np.mean((w - w_)**2)))\n",
    "    return w\n",
    "\n",
    "def evaluate_policy(pi, pi0, y, clip=None, bootstraps:int=None, self_normalized=True, verbose=False):\n",
    "    \"\"\"\n",
    "    Estimate E_{\\pi}[Y] using importance sampling.\n",
    "    \n",
    "    - pi = proba to take action under new policy (vector of size n_samples)\n",
    "    - pi0 = proba to take action under original policy (vector of size n_samples)\n",
    "    - y = historical rewards (vector of size n_samples)\n",
    "    \n",
    "    if nb_bootstraps is not None, a bootstrap distribution of E_{\\pi}[Y] is returned (array of size nb_bootstraps)\n",
    "    otherwise the mean is returned (a scalar)\n",
    "    \"\"\"\n",
    "    \n",
    "    w = pi / pi0\n",
    "    \n",
    "    normalizer = lambda x: x / np.mean(w) if self_normalized else x\n",
    "\n",
    "    if verbose:\n",
    "        print(\"E[pi0]:\", np.mean(pi0))\n",
    "        print(\"E[pi']:\", np.mean(pi))\n",
    "        print(\"E[W]:\", np.mean(w))\n",
    "        print(\"E[W~]:\", np.mean(normalizer(w)))\n",
    "    \n",
    "    w = normalizer(clip_weights(w, clip=clip, verbose=verbose))\n",
    "    \n",
    "    if bootstraps is not None:\n",
    "        return [\n",
    "                np.dot(\n",
    "                    np.multiply(\n",
    "                        w,\n",
    "                        np.random.poisson(lam=1, size=len(y))\n",
    "                    ), \n",
    "                    y\n",
    "                ) / len(y)\n",
    "            for _ in range(bootstraps)\n",
    "        ]\n",
    "    else:\n",
    "        return np.dot(w, y) / len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concluding remarks:\n",
    "- this procedure could be turned into a real optimization (e.g. by computing the gradient of $\\alpha$)\n",
    "- it could be made contextual by optimizing a different policy for each context "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
